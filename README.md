# üß¨ MitoSAM-ViT

<a target="_blank" href="https://cookiecutter-data-science.drivendata.org/">
    <img src="https://img.shields.io/badge/CCDS-Project%20template-328F97?logo=cookiecutter" />
</a>
<img src="https://img.shields.io/badge/status-work%20in%20progress-orange" />

A work-in-progress project to adapt the **Segment Anything Model (SAM)** to **mitochondria segmentation** in 3D electron microscopy (EM) data using **LoRA** and **PEFT adapters**.

---

## üî¨ Project overview

**MitoSAM-ViT** explores whether a general-purpose segmentation foundation model, **Segment Anything** (SAM) (Kirillov *et al.*, 2023), can be efficiently specialized for **mitochondria** in EM images using **parameter-efficient fine-tuning**.

Current focus:

- Fine-tuning a **SAM ViT backbone** with **LoRA adapters** using the **PEFT** library.
- Training on a **mitochondria EM dataset from EPFL**, with voxel-level annotations.

The long-term goal is a **fully automated mitochondria segmentation pipeline** that combines:

- **YOLOv8**-based object detection for mitochondria; and  
- **SAM**, prompted with YOLO bounding boxes, for refined instance segmentation.

This project is conceptually related to:
- **Segment Anything (SAM)** ‚Äì general promptable segmentation.  
- **Segment Anything for Microscopy (MicroSAM / ŒºSAM)** ‚Äì SAM-based tools for microscopy segmentation and tracking.  
- **Ultralytics YOLOv8** ‚Äì modern real-time object detector used here for mitochondria localization.

---

## üß† Method (current stage)

1. **Base model**
   - Segment Anything Model (SAM) image encoder + mask decoder.
   - Promptable segmentation via points / boxes / masks.

2. **Parameter-efficient fine-tuning**
   - Insert **LoRA** adapters into selected Transformer layers of SAM‚Äôs ViT encoder.
   - Keep original SAM weights frozen; train only the low-rank adapter parameters.
   - Use **Hugging Face PEFT** to manage LoRA adapters (config, loading/saving, experiments).

3. **Data**
   - Mitochondria EM dataset from **EPFL** (3D volume EM with expert mitochondria annotations).
   - Standard split into training / validation volumes, with pre-processing handled in the `data/` and `mitosam/dataset.py` pipeline.

---

## üó∫Ô∏è Planned roadmap

This repository is **under active development**. Planned next steps:

1. **Mitochondria detection with YOLOv8**
   - Train a **YOLOv8** detector on EM images to predict **bounding boxes** for mitochondria.
   - Optimize for high recall to ensure most mitochondria are detected.

2. **Detection-guided SAM segmentation**
   - Use YOLOv8 **bounding boxes as prompts** for SAM.
   - For each detected box, SAM refines the region into a high-quality segmentation mask.
   - This forms a **detection + promptable segmentation pipeline** for automated mitochondria instance segmentation.

3. **Evaluation & analysis**
   - Compare:
     - Zero-shot SAM vs. LoRA-fine-tuned SAM.
     - Bounding-box-only detection vs. YOLOv8 + SAM masks.
   - Report metrics such as IoU / Dice for both semantic and instance-level mitochondria segmentation.

---

## üìÅ Project Organization

This project follows the [cookiecutter-data-science](https://cookiecutter-data-science.drivendata.org/) layout:

```text
‚îú‚îÄ‚îÄ LICENSE            <- Open-source license if one is chosen
‚îú‚îÄ‚îÄ Makefile           <- Makefile with convenience commands like `make data` or `make train`
‚îú‚îÄ‚îÄ README.md          <- The top-level README for developers using this project.
‚îú‚îÄ‚îÄ data
‚îÇ   ‚îú‚îÄ‚îÄ external       <- Data from third party sources.
‚îÇ   ‚îú‚îÄ‚îÄ interim        <- Intermediate data that has been transformed.
‚îÇ   ‚îú‚îÄ‚îÄ processed      <- The final, canonical data sets for modeling.
‚îÇ   ‚îî‚îÄ‚îÄ raw            <- The original, immutable data dump.
‚îÇ
‚îú‚îÄ‚îÄ docs               <- A default mkdocs project; see www.mkdocs.org for details
‚îÇ
‚îú‚îÄ‚îÄ models             <- Trained and serialized models, model predictions, or model summaries
‚îÇ
‚îú‚îÄ‚îÄ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
‚îÇ                         the creator's initials, and a short `-` delimited description, e.g.
‚îÇ                         `1.0-jqp-initial-data-exploration`.
‚îÇ
‚îú‚îÄ‚îÄ pyproject.toml     <- Project configuration file with package metadata for 
‚îÇ                         mitosam and configuration for tools like black
‚îÇ
‚îú‚îÄ‚îÄ references         <- Data dictionaries, manuals, and all other explanatory materials.
‚îÇ
‚îú‚îÄ‚îÄ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
‚îÇ   ‚îî‚îÄ‚îÄ figures        <- Generated graphics and figures to be used in reporting
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
‚îÇ                         generated with `pip freeze > requirements.txt`
‚îÇ
‚îú‚îÄ‚îÄ setup.cfg          <- Configuration file for flake8
‚îÇ
‚îî‚îÄ‚îÄ mitosam   <- Source code for use in this project.
    ‚îÇ
    ‚îú‚îÄ‚îÄ __init__.py             <- Makes mitosam a Python module
    ‚îÇ
    ‚îú‚îÄ‚îÄ config.py               <- Store useful variables and configuration
    ‚îÇ
    ‚îú‚îÄ‚îÄ dataset.py              <- Scripts to download or generate data
    ‚îÇ
    ‚îú‚îÄ‚îÄ features.py             <- Code to create features for modeling
    ‚îÇ
    ‚îú‚îÄ‚îÄ modeling                
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py 
    ‚îÇ   ‚îú‚îÄ‚îÄ predict.py          <- Code to run model inference with trained models          
    ‚îÇ   ‚îî‚îÄ‚îÄ train.py            <- Code to train models
    ‚îÇ
    ‚îî‚îÄ‚îÄ plots.py                <- Code to create visualizations



```
## üöß Status

This is **research code** and is **actively evolving**.  
Interfaces, training scripts, and experiment configurations may change as the project progresses.



## üìö References

- **Segment Anything (SAM)**  
  A. Kirillov *et al.* ‚ÄúSegment Anything.‚Äù *ICCV*, 2023. [arXiv:2304.02643](https://arxiv.org/abs/2304.02643)

- **Segment Anything for Microscopy (MicroSAM / ŒºSAM)**  
  A. Archit *et al.* ‚ÄúSegment Anything for Microscopy (ŒºSAM).‚Äù *Nature Methods*, 2025. [Article](https://www.nature.com/articles/s41592-024-02580-4)

- **Ultralytics YOLOv8**  
  Ultralytics. ‚ÄúUltralytics YOLOv8: State-of-the-Art Computer Vision Model.‚Äù [Documentation](https://docs.ultralytics.com/models/yolov8/)

- **EPFL Electron Microscopy (CA1 Hippocampus) Dataset**  
  EPFL CVLAB. ‚ÄúElectron Microscopy Dataset (CA1 hippocampus, mitochondria annotations).‚Äù [Dataset page](https://www.epfl.ch/labs/cvlab/data/data-em/)

